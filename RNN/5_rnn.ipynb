{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75757a75f295a7abc1193a8474830e51",
     "grade": false,
     "grade_id": "cell-f5e46023398b0aab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 4\n",
    "<br>\n",
    "<b>Deadline:</b> March 30, 2020 (Monday). 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 5. Sequence-to-sequence modeling with recurrent neural networks\n",
    "\n",
    "The goals of this exercise are\n",
    "* to get familiar with recurrent neural networks used for sequential data processing\n",
    "* to get familiar with the sequence-to-sequence model for machine translation\n",
    "* to learn PyTorch tools for batch processing of sequences with varying lengths\n",
    "* to learn how to write a custom `DataLoader`\n",
    "\n",
    "You may find it useful to look at this tutorial:\n",
    "* [Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbbca8fe9cf0cb1cb20dd200e23cfcb0",
     "grade": false,
     "grade_id": "cell-44cf6f3242607cde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88cb529e1e7d2069f68d3df82a852cce",
     "grade": false,
     "grade_id": "cell-1f1e529682d7ce6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset that we are going to use consists of pairs of sentences in French and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa7408c0ea947cac397f5e35ce6498ee",
     "grade": false,
     "grade_id": "cell-6638bcc556bb02f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from data import TranslationDataset, MAX_LENGTH, SOS_token, EOS_token\n",
    "\n",
    "trainset = TranslationDataset(data_dir, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac3d437d5ade200a6ff8ca5407bf2c01",
     "grade": false,
     "grade_id": "cell-2d3c1dd7e239d2fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* `TranslationDataset` supports indexing as required by `torch.utils.data.Dataset`.\n",
    "* Sentences are tensors of maximum length `MAX_LENGTH`.\n",
    "* Words in a (sentence) tensor are represented as an index (integer) in a language vocabulary.\n",
    "* The string representation of a word from the source language can be obtained from index `i` with `dataset.input_lang.index2word[i]`.\n",
    "* Similarly for the target language `dataset.output_lang.index2word[j]`.\n",
    "\n",
    "Let us look at samples from that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b55856502471f9f6a7f4a7f3699504b5",
     "grade": false,
     "grade_id": "cell-0d793aaf021670ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: \"je t en supplie . EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([   6,  218,   14, 1143,    5,    1])\n",
      "Target sentence: \"i m begging you . EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([  2,   3, 620, 130,   4,   1])\n"
     ]
    }
   ],
   "source": [
    "src_sentence, tgt_sentence = trainset[np.random.choice(len(trainset))]\n",
    "print('Source sentence: \"%s\"' % ' '.join(trainset.input_lang.index2word[i.item()] for i in src_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(src_sentence)\n",
    "\n",
    "print('Target sentence: \"%s\"' % ' '.join(trainset.output_lang.index2word[i.item()] for i in tgt_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(tgt_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d8116da735b2c5503a947814fd393b0",
     "grade": false,
     "grade_id": "cell-94d57799bcd1786b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source-target pairs in the training set:  8682\n"
     ]
    }
   ],
   "source": [
    "print('Number of source-target pairs in the training set: ', len(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c5eec5bf6f7b88396659a1cdc5faa2c",
     "grade": false,
     "grade_id": "cell-80e91375dcb62ed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sequence-to-sequence model for machine translation\n",
    "\n",
    "In this exercise, we are going to build a machine translation system which transforms a sentence in one language into a sentence in another one. The computational graph of the translation model is shown below:\n",
    "\n",
    "<img src=\"seq2seq.png\" width=900>\n",
    "\n",
    "We are going to use a simplified model without the dotted connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c7dbcc1965af7880daaa76d42b796f9",
     "grade": false,
     "grade_id": "cell-86482ed71ea81ed3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Custom DataLoader\n",
    "\n",
    "We would like to train the sequence-to-sequence model using mini-batch training.\n",
    "One difficulty of mini-batch training in this case is that sequences may have varying lengths and this has to be taken into account when building the computational graph. Luckily, PyTorch has tools to support batch processing of such sequences.\n",
    "To use those tools, we need to write a custom data loader which puts sequences of varying lengths in the same tensor. We can customize the data loader by providing a custom `collate_fn` as explained [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Our collate function:\n",
    "- combines sequences from the source language in a single tensor with extra values (at the end) filled with `padding_value=0`.\n",
    "- combines sequences from the target language in a single tensor with extra values (at the end) filled with `padding_value=0`.\n",
    "\n",
    "**Important**:\n",
    "- Late in the code (not in this `collate` function), we will convert source sequences to objects of class [`PackedSequence`](https://pytorch.org/docs/stable/nn.html?highlight=packedsequence#torch.nn.utils.rnn.PackedSequence) which can be processed by recurrent units such as `GRU` or `LSTM`. `PackedSequence` requires sequences to be sorted by their lengths.\n",
    "**Therefore, the returned source sequences should be sorted by length in a decreasing order.**\n",
    "* The target sequences need not be sorted by their lengths because we have to keep the same order of sequences in the source and target tensors.\n",
    "\n",
    "Your task is to implement the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab58f007e4eb51017581a1594819b1c6",
     "grade": false,
     "grade_id": "cell-f7cf358c436c6d49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "padding_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d791959e8c9e2b324dec5a664ab99b26",
     "grade": false,
     "grade_id": "collate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "    Args:\n",
    "      list_of_samples is a list of tuples (src_seq, tgt_seq):\n",
    "          src_seq is of shape (src_seq_length,)\n",
    "          tgt_seq is of shape (tgt_seq_length,)\n",
    "\n",
    "    Returns:\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "          The sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "          the longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "      src_seq_lengths: List of lengths of source sequences.\n",
    "      tgt_seqs of shape (max_tgt_seq_length, batch_size): Tensor of padded target sequences.\n",
    "    \"\"\"\n",
    "    src_list=[];\n",
    "    tgt_list=[];\n",
    "    src_seq_len=[];\n",
    "    for i in list_of_samples:\n",
    "        src=i[0]\n",
    "        tgt=i[1]\n",
    "        src_seq_len.append(len(src))\n",
    "        src_list.append(src)\n",
    "        tgt_list.append(tgt)\n",
    "    src_list=sorted(src_list, key=len,reverse=True) \n",
    "    #src_list.sort(key=lambda x: len(x[0]))\n",
    "    #src_list.sort(key=lambda t: len(t[1]), reverse=True)\n",
    "    src_seqs=pad_sequence(src_list, batch_first=False, padding_value=0)\n",
    "    tgt_seq=pad_sequence(tgt_list, batch_first=False, padding_value=0)\n",
    "    return src_seqs, src_seq_len, tgt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acf9b6ec3b17cfc139afb02a653e77e0",
     "grade": false,
     "grade_id": "cell-946d5fa69247a122",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), torch.LongTensor([3, 4, 5])),\n",
    "        (torch.LongTensor([6, 7, 8]), torch.LongTensor([9, 10])),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3, 2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0efe605cfa9e99f34c1fd6ab713d5a1d",
     "grade": true,
     "grade_id": "test_collate_fn",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests collate() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b934c76a58a2f6d3316fdf96cdee26e0",
     "grade": false,
     "grade_id": "cell-b4fb631ef92b42cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea1cc5c8b8d4736b25686e444c996848",
     "grade": false,
     "grade_id": "cell-3f6dfc8dc7015270",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder encodes a source sequence $(x_1, x_2, ..., x_T)$ into a single vector $h_T$ using the following recursion:\n",
    "$$\n",
    "  h_{t} = f(h_{t-1}, x_t) \\qquad t = 1, \\ldots, T\n",
    "$$\n",
    "where:\n",
    "* intial state $h_0$ is often chosen arbitrarily (we choose it to be zero)\n",
    "* function $f$ is defined by the type of the RNN cell (in our experiments, we will use [GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU))\n",
    "* $x_t$ is a vector that represents the $t$-th word in the source sentence.\n",
    "\n",
    "A common practice in natural language processing is to _learn_ the word representations $x_t$ (instead of, for example, using one-hot coded vectors). In PyTorch, this is supported by class [Embedding](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) which we are going to use.\n",
    "\n",
    "The computational graph of the encoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_encoder.png\" width=500>\n",
    "\n",
    "Your task is to implement the `forward` function of the encoder. It should contain the following steps:\n",
    "* Embed the words of the source sequences.\n",
    "* Pack source sequences using [`pack_padded_sequence`](https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence). This converts padded source sequences into an object that can be processed by PyTorch recurrent units such as `nn.GRU` or `nn.LSTM`.\n",
    "* Apply GRU computations to packed sequences obtained in the previous step\n",
    "* Convert packed sequence of GRU outputs into padded representation with [`pad_packed_sequence`](https://pytorch.org/docs/stable/nn.html?highlight=pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad27026a59c87defe4742f2afbcabd4",
     "grade": false,
     "grade_id": "Encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_dictionary_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src_dictionary_size: The number of words in the source dictionary.\n",
    "          embed_size: The number of dimensions in the word embeddings.\n",
    "          hidden_size: The number of features in the hidden state of GRU.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "        self.gru = nn.GRU(input_size=embed_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          pad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "          seq_lengths: List of sequence lengths.\n",
    "          hidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "        Returns:\n",
    "          outputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "          hidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x=self.embedding(pad_seqs)\n",
    "        x=pack_padded_sequence(x,seq_lengths,batch_first=False)\n",
    "        outputs, new_hidden = self.gru(x, hidden)\n",
    "        \n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=False)\n",
    "        return outputs, new_hidden\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da4ed01168950d985ee570f9d442d001",
     "grade": false,
     "grade_id": "cell-33422e03c9970c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Encoder_shapes():\n",
    "    hidden_size = 3\n",
    "    encoder = Encoder(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = encoder.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    outputs, new_hidden = encoder.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([4, batch_size, hidden_size]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape: {new_hidden.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52216851f11f2e4399a58cb059c61d6b",
     "grade": true,
     "grade_id": "test_Encoder",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[:, 0, :]:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0221],\n",
      "        [ 0.0007, -0.0055],\n",
      "        [ 0.0005,  0.0323]])\n",
      "expected:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0221],\n",
      "        [ 0.0007, -0.0055],\n",
      "        [ 0.0005,  0.0323]])\n",
      "outputs[:2, 1, :]:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0021]])\n",
      "expected:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0021]])\n",
      "new_hidden:\n",
      " tensor([[[ 0.0005,  0.0323],\n",
      "         [ 0.0004, -0.0021]]])\n",
      "expected:\n",
      " tensor([[[ 0.0005,  0.0323],\n",
      "         [ 0.0004, -0.0021]]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "tests.test_Encoder(Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10c7cd999816f36facde0e1d3687c9b4",
     "grade": false,
     "grade_id": "cell-3133e50590987e56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder takes as input the representation computed by the encoder and transforms it into a sentence in the target language. The computational graph of the decoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_decoder.png\" width=500 align=\"top\">\n",
    "\n",
    "* $z_0$ is the output of the encoder, that is $z_0 = h_5$, thus `hidden_size` of the decoder should be the same as `hidden_size` of the encoder.\n",
    "* $y_{i}$ are the log-probabilities of the words in the target language, the dimensionality of $y_{i}$ is the size of the target dictionary.\n",
    "* $z_{i}$ is mapped to $y_{i}$ using a linear layer `self.out` followed by `F.log_softmax` (because we use `nn.NLLLoss` loss for training).\n",
    "* Each cell of the decoder is a GRU, it receives as inputs the previous state $z_{i-1}$ and relu of the **embedding** of the previous word. Thus, you need to embed the words of the target language as well. The previous word is taken as the word with the maximum log-probability.\n",
    "\n",
    "Note that the decoder outputs a word at every step and the same word is used as the input to the recurrent unit at the next step. At the beginning of decoding, the previous word input is fed with a special word SOS which stands for \"start of a sentence\". During training, we know the target sentence for decoding, therefore we can feed the correct words $y_i$ as inputs to the recurrent unit.\n",
    "\n",
    "There is one extra thing that it is wise to take care of. When the target sentence is fed to the decoder during training, the decoder learns to generate only the next word (this scenario is called \"teacher forcing\"). At test time, the decoder works differently: it generates the whole sequence using its own predictions as inputs at each step. Therefore, it makes sense to train the decoder to produce full sentences. In order to do that, we will alternate between two modes during training:\n",
    "* \"teacher forcing\": the decoder is fed with the words in the target sequence\n",
    "* no \"teacher forcing\": the decoder generates the output sequence using its own predictions. In this case, we will generate sequences of the same length as the length of the longest sequence in `pad_tgt_seqs` (if `pad_tgt_seqs` is not `None`) or of length `MAX_LENGTH` (if `pad_tgt_seqs` is `None`).\n",
    "\n",
    "You need to implement the decoder which has the structure shown in the figure above.\n",
    "\n",
    "Note:\n",
    "* `SOS_token` is imported at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70b2c4f47ee96519de22564de0a2f192",
     "grade": false,
     "grade_id": "Decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_dictionary_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt_dictionary_size: The number of words in the target dictionary.\n",
    "          embed_size: The number of dimensions in the word embeddings.\n",
    "          hidden_size: The number of features in the hidden state.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(tgt_dictionary_size, embed_size)\n",
    "        self.gru = nn.GRU(input_size=embed_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, tgt_dictionary_size)\n",
    "        #self.tgt_dictionary_size = tgt_dictionary_size\n",
    "        #self.softmax = nn.LogSoftmax(dim = 2)\n",
    "\n",
    "    def forward(self, hidden, pad_tgt_seqs=None, teacher_forcing=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden of shape (1, batch_size, hidden_size): States of the GRU.\n",
    "          pad_tgt_seqs of shape (max_out_seq_length, batch_size): Tensor of words (word indices) of the\n",
    "              target sentence. If None, the output sequence is generated by feeding the decoder's outputs\n",
    "              (teacher_forcing has to be False).\n",
    "          teacher_forcing (bool): Whether to use teacher forcing or not.\n",
    "\n",
    "        Returns:\n",
    "          outputs of shape (max_out_seq_length, batch_size, tgt_dictionary_size): Tensor of log-probabilities\n",
    "              of words in the target language.\n",
    "          hidden of shape (1, batch_size, hidden_size): New states of the GRU.\n",
    "\n",
    "        Note: Do not forget to transfer tensors that you may want to create in this function to the device\n",
    "        specified by `hidden.device`.\n",
    "        \"\"\"\n",
    "        if pad_tgt_seqs is None:\n",
    "            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'\n",
    "            \n",
    "        prev_word = torch.tensor(SOS_token * np.ones((1, hidden.size(1))), device = device, dtype = torch.int64)\n",
    "        out_length = pad_tgt_seqs.size(0) if pad_tgt_seqs is not None else MAX_LENGTH\n",
    "        outputs = []\n",
    "        # YOUR CODE HERE\n",
    "        for i in range(out_length):\n",
    "            embedded = self.embedding(prev_word.view(1, -1))\n",
    "            #print(embedded)\n",
    "            #embedded = embedded.view(self.hidden_size)\n",
    "            output = F.relu(embedded)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "            output = self.out(output)\n",
    "            output= F.log_softmax(output, dim=2)\n",
    "            outputs.append(output)\n",
    "            #print(outputs.shape)\n",
    "            if teacher_forcing:\n",
    "                prev_word = pad_tgt_seqs[i]\n",
    "                \n",
    "            else: \n",
    "                prev_word =  torch.max(output, dim=2)[1].detach()\n",
    "                # Use its own predictions as the next input\n",
    "                #topv, topi = output[0, :].topk(1)\n",
    "                #prev_word = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                #if prev_word.item() == EOS_token:\n",
    "                 #   break\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)  # [max_length, batch_size, output_dictionary_size]        \n",
    "        return outputs, hidden\n",
    "    \n",
    "    #def init_hidden(self, batch_size=1):\n",
    "        #return torch.zeros(1, batch_size, self.hidden_size)\n",
    "        #output, hidden = self.gru()\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44ee557807a8d6b49c670683e93e0172",
     "grade": false,
     "grade_id": "cell-f0749cc463edb855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Decoder_shapes():\n",
    "    hidden_size = 2\n",
    "    tgt_dictionary_size = 5\n",
    "    test_decoder = Decoder(tgt_dictionary_size, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    pad_tgt_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])  # [max_seq_length, batch_size]\n",
    "\n",
    "    hidden = torch.zeros(1, batch_size, hidden_size)\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, pad_tgt_seqs, teacher_forcing=False)\n",
    "\n",
    "    assert outputs.size(0) <= 4, f\"Too long output sequence: outputs.size(0)={outputs.size(0)}\"\n",
    "    assert outputs.shape[1:] == torch.Size([batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad outputs.shape[1:]={outputs.shape[1:]}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, pad_tgt_seqs, teacher_forcing=True)\n",
    "    assert outputs.shape == torch.Size([4, batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad shape outputs.shape={outputs.shape}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    # Generation mode\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, None, teacher_forcing=False)\n",
    "    assert outputs.shape[1:] == torch.Size([batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad outputs.shape[1:]={outputs.shape[1:]}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "146622a450e813afc6a18832d11aa9d2",
     "grade": true,
     "grade_id": "test_Decoder",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[:, 0, :]:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956]])\n",
      "expected:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956]])\n",
      "outputs[:, 1, :]:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956]])\n",
      "expected:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956]])\n",
      "new_hidden:\n",
      " tensor([[[0.1003, 0.0421],\n",
      "         [0.1003, 0.0421]]])\n",
      "expected:\n",
      " tensor([[[0.1003, 0.0421],\n",
      "         [0.1003, 0.0421]]])\n",
      "Success\n",
      "outputs[:, 0, :]:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3414, -1.8877, -1.5123, -1.7854, -1.6146],\n",
      "        [-1.4662, -1.7419, -1.5607, -1.6986, -1.6040],\n",
      "        [-1.5418, -1.6619, -1.5932, -1.6532, -1.6019]])\n",
      "expected:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3414, -1.8877, -1.5123, -1.7854, -1.6146],\n",
      "        [-1.4662, -1.7419, -1.5607, -1.6986, -1.6040],\n",
      "        [-1.5418, -1.6619, -1.5932, -1.6532, -1.6019]])\n",
      "outputs[:, 1, :]:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3398, -1.8909, -1.5107, -1.7862, -1.6154],\n",
      "        [-1.4706, -1.7343, -1.5652, -1.6971, -1.6024],\n",
      "        [-1.5557, -1.6402, -1.6069, -1.6492, -1.5979]])\n",
      "expected:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3398, -1.8909, -1.5107, -1.7862, -1.6154],\n",
      "        [-1.4706, -1.7343, -1.5652, -1.6971, -1.6024],\n",
      "        [-1.5557, -1.6402, -1.6069, -1.6492, -1.5979]])\n",
      "new_hidden:\n",
      " tensor([[[ 0.1028, -0.0173],\n",
      "         [ 0.1025,  0.0180]]])\n",
      "expected:\n",
      " tensor([[[ 0.1028, -0.0173],\n",
      "         [ 0.1025,  0.0180]]])\n",
      "Success\n",
      "outputs[:, 0, :]:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956],\n",
      "        [-1.6074, -1.5869, -1.6333, -1.6230, -1.5972],\n",
      "        [-1.6125, -1.5923, -1.6252, -1.6151, -1.6024],\n",
      "        [-1.6151, -1.5950, -1.6212, -1.6111, -1.6050],\n",
      "        [-1.6164, -1.5963, -1.6192, -1.6091, -1.6064],\n",
      "        [-1.6170, -1.5970, -1.6182, -1.6081, -1.6070],\n",
      "        [-1.6173, -1.5973, -1.6177, -1.6077, -1.6073]])\n",
      "expected:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956],\n",
      "        [-1.6074, -1.5869, -1.6333, -1.6230, -1.5972],\n",
      "        [-1.6125, -1.5923, -1.6252, -1.6151, -1.6024],\n",
      "        [-1.6151, -1.5950, -1.6212, -1.6111, -1.6050],\n",
      "        [-1.6164, -1.5963, -1.6192, -1.6091, -1.6064],\n",
      "        [-1.6170, -1.5970, -1.6182, -1.6081, -1.6070],\n",
      "        [-1.6173, -1.5973, -1.6177, -1.6077, -1.6073]])\n",
      "outputs[:, 1, :]:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956],\n",
      "        [-1.6074, -1.5869, -1.6333, -1.6230, -1.5972],\n",
      "        [-1.6125, -1.5923, -1.6252, -1.6151, -1.6024],\n",
      "        [-1.6151, -1.5950, -1.6212, -1.6111, -1.6050],\n",
      "        [-1.6164, -1.5963, -1.6192, -1.6091, -1.6064],\n",
      "        [-1.6170, -1.5970, -1.6182, -1.6081, -1.6070],\n",
      "        [-1.6173, -1.5973, -1.6177, -1.6077, -1.6073]])\n",
      "expected:\n",
      " tensor([[-1.1366, -2.1924, -1.4361, -1.9640, -1.6645],\n",
      "        [-1.3540, -1.8630, -1.5249, -1.7793, -1.6085],\n",
      "        [-1.4899, -1.7024, -1.5838, -1.6901, -1.5962],\n",
      "        [-1.5665, -1.6246, -1.6166, -1.6457, -1.5956],\n",
      "        [-1.6074, -1.5869, -1.6333, -1.6230, -1.5972],\n",
      "        [-1.6125, -1.5923, -1.6252, -1.6151, -1.6024],\n",
      "        [-1.6151, -1.5950, -1.6212, -1.6111, -1.6050],\n",
      "        [-1.6164, -1.5963, -1.6192, -1.6091, -1.6064],\n",
      "        [-1.6170, -1.5970, -1.6182, -1.6081, -1.6070],\n",
      "        [-1.6173, -1.5973, -1.6177, -1.6077, -1.6073]])\n",
      "new_hidden:\n",
      " tensor([[[0.0006, 0.0207],\n",
      "         [0.0006, 0.0207]]])\n",
      "expected:\n",
      " tensor([[[0.0006, 0.0207],\n",
      "         [0.0006, 0.0207]]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "tests.test_Decoder_no_forcing(Decoder)\n",
    "tests.test_Decoder_with_forcing(Decoder)\n",
    "tests.test_Decoder_generation(Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "876b987bce981f6692a1847ba12a888f",
     "grade": false,
     "grade_id": "cell-6207d3c96c443b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Training of sequence-to-sequence model using mini-batches\n",
    "\n",
    "Now we are going to train the sequence-to-sequence model on the toy translation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a4c7142fd6d3aea8ec9bd599f32b68",
     "grade": false,
     "grade_id": "cell-dc6ed9b2b10473c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the seq2seq model\n",
    "hidden_size = embed_size = 256\n",
    "encoder = Encoder(trainset.input_lang.n_words, embed_size, hidden_size).to(device)\n",
    "decoder = Decoder(trainset.output_lang.n_words, embed_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529e79ae57187958b239781c17b99635",
     "grade": false,
     "grade_id": "cell-6ce059e5ee375d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e9e114dd42a8574cf0f0147ac83ab9f",
     "grade": false,
     "grade_id": "cell-b1645c0797a9d5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Implement the training loop in the cell below. In the training loop, we first encode source sequences using the encoder, then we decode the encoded state using the decoder. The decoder outputs log-probabilities of words in the target language. We need to use these log-probabilities and the indexes of the words in the target sequences to compute the loss.\n",
    "\n",
    "Recommended hyperparameters:\n",
    "- Encoder optimizer: Adam with learning rate 0.001\n",
    "- Decoder optimizer: Adam with learning rate 0.001\n",
    "- Number of epochs: 30\n",
    "- Toggle `teacher_forcing` on and off (for each mini-batch) according to the `teacher_forcing_ratio` specified above.\n",
    "\n",
    "Hints:\n",
    "- Training should proceed relatively fast.\n",
    "- If you do well, the training loss should reach 0.1 in 30 epochs.\n",
    "- **Important:** When computing the loss, you need to ignore the padded values. This can easily be done by using argument `ignore_index` of function [`nll_loss`](\n",
    "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7a530b833c54e4e7de2bac7b3b235c0",
     "grade": false,
     "grade_id": "cell-39d518fd8074b9ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/030 | Batch 0001/0136 | Loss: 0.2460\n",
      "Epoch: 001/030 | Batch 0033/0136 | Loss: 4.1450\n",
      "Epoch: 001/030 | Batch 0065/0136 | Loss: 2.9673\n",
      "Epoch: 001/030 | Batch 0097/0136 | Loss: 2.7792\n",
      "Epoch: 001/030 | Batch 0129/0136 | Loss: 2.6186\n",
      "Epoch: 002/030 | Batch 0001/0136 | Loss: 0.0709\n",
      "Epoch: 002/030 | Batch 0033/0136 | Loss: 2.3656\n",
      "Epoch: 002/030 | Batch 0065/0136 | Loss: 2.2979\n",
      "Epoch: 002/030 | Batch 0097/0136 | Loss: 2.2329\n",
      "Epoch: 002/030 | Batch 0129/0136 | Loss: 2.2157\n",
      "Epoch: 003/030 | Batch 0001/0136 | Loss: 0.0693\n",
      "Epoch: 003/030 | Batch 0033/0136 | Loss: 2.0440\n",
      "Epoch: 003/030 | Batch 0065/0136 | Loss: 2.1144\n",
      "Epoch: 003/030 | Batch 0097/0136 | Loss: 2.0339\n",
      "Epoch: 003/030 | Batch 0129/0136 | Loss: 1.9624\n",
      "Epoch: 004/030 | Batch 0001/0136 | Loss: 0.0482\n",
      "Epoch: 004/030 | Batch 0033/0136 | Loss: 1.8406\n",
      "Epoch: 004/030 | Batch 0065/0136 | Loss: 1.8305\n",
      "Epoch: 004/030 | Batch 0097/0136 | Loss: 1.7751\n",
      "Epoch: 004/030 | Batch 0129/0136 | Loss: 1.8114\n",
      "Epoch: 005/030 | Batch 0001/0136 | Loss: 0.0594\n",
      "Epoch: 005/030 | Batch 0033/0136 | Loss: 1.6499\n",
      "Epoch: 005/030 | Batch 0065/0136 | Loss: 1.6984\n",
      "Epoch: 005/030 | Batch 0097/0136 | Loss: 1.6561\n",
      "Epoch: 005/030 | Batch 0129/0136 | Loss: 1.6096\n",
      "Epoch: 006/030 | Batch 0001/0136 | Loss: 0.0527\n",
      "Epoch: 006/030 | Batch 0033/0136 | Loss: 1.4571\n",
      "Epoch: 006/030 | Batch 0065/0136 | Loss: 1.4707\n",
      "Epoch: 006/030 | Batch 0097/0136 | Loss: 1.5057\n",
      "Epoch: 006/030 | Batch 0129/0136 | Loss: 1.4556\n",
      "Epoch: 007/030 | Batch 0001/0136 | Loss: 0.0390\n",
      "Epoch: 007/030 | Batch 0033/0136 | Loss: 1.4381\n",
      "Epoch: 007/030 | Batch 0065/0136 | Loss: 1.3804\n",
      "Epoch: 007/030 | Batch 0097/0136 | Loss: 1.3283\n",
      "Epoch: 007/030 | Batch 0129/0136 | Loss: 1.3852\n",
      "Epoch: 008/030 | Batch 0001/0136 | Loss: 0.0329\n",
      "Epoch: 008/030 | Batch 0033/0136 | Loss: 1.1979\n",
      "Epoch: 008/030 | Batch 0065/0136 | Loss: 1.2018\n",
      "Epoch: 008/030 | Batch 0097/0136 | Loss: 1.2293\n",
      "Epoch: 008/030 | Batch 0129/0136 | Loss: 1.1957\n",
      "Epoch: 009/030 | Batch 0001/0136 | Loss: 0.0271\n",
      "Epoch: 009/030 | Batch 0033/0136 | Loss: 1.0320\n",
      "Epoch: 009/030 | Batch 0065/0136 | Loss: 1.1360\n",
      "Epoch: 009/030 | Batch 0097/0136 | Loss: 1.0576\n",
      "Epoch: 009/030 | Batch 0129/0136 | Loss: 1.1095\n",
      "Epoch: 010/030 | Batch 0001/0136 | Loss: 0.0348\n",
      "Epoch: 010/030 | Batch 0033/0136 | Loss: 0.9439\n",
      "Epoch: 010/030 | Batch 0065/0136 | Loss: 0.9852\n",
      "Epoch: 010/030 | Batch 0097/0136 | Loss: 0.9796\n",
      "Epoch: 010/030 | Batch 0129/0136 | Loss: 1.0032\n",
      "Epoch: 011/030 | Batch 0001/0136 | Loss: 0.0262\n",
      "Epoch: 011/030 | Batch 0033/0136 | Loss: 0.8528\n",
      "Epoch: 011/030 | Batch 0065/0136 | Loss: 0.8461\n",
      "Epoch: 011/030 | Batch 0097/0136 | Loss: 0.8600\n",
      "Epoch: 011/030 | Batch 0129/0136 | Loss: 0.8858\n",
      "Epoch: 012/030 | Batch 0001/0136 | Loss: 0.0289\n",
      "Epoch: 012/030 | Batch 0033/0136 | Loss: 0.7359\n",
      "Epoch: 012/030 | Batch 0065/0136 | Loss: 0.7387\n",
      "Epoch: 012/030 | Batch 0097/0136 | Loss: 0.7943\n",
      "Epoch: 012/030 | Batch 0129/0136 | Loss: 0.7814\n",
      "Epoch: 013/030 | Batch 0001/0136 | Loss: 0.0146\n",
      "Epoch: 013/030 | Batch 0033/0136 | Loss: 0.6591\n",
      "Epoch: 013/030 | Batch 0065/0136 | Loss: 0.6198\n",
      "Epoch: 013/030 | Batch 0097/0136 | Loss: 0.6974\n",
      "Epoch: 013/030 | Batch 0129/0136 | Loss: 0.6796\n",
      "Epoch: 014/030 | Batch 0001/0136 | Loss: 0.0176\n",
      "Epoch: 014/030 | Batch 0033/0136 | Loss: 0.5506\n",
      "Epoch: 014/030 | Batch 0065/0136 | Loss: 0.6112\n",
      "Epoch: 014/030 | Batch 0097/0136 | Loss: 0.5553\n",
      "Epoch: 014/030 | Batch 0129/0136 | Loss: 0.5970\n",
      "Epoch: 015/030 | Batch 0001/0136 | Loss: 0.0138\n",
      "Epoch: 015/030 | Batch 0033/0136 | Loss: 0.4896\n",
      "Epoch: 015/030 | Batch 0065/0136 | Loss: 0.4933\n",
      "Epoch: 015/030 | Batch 0097/0136 | Loss: 0.5022\n",
      "Epoch: 015/030 | Batch 0129/0136 | Loss: 0.5097\n",
      "Epoch: 016/030 | Batch 0001/0136 | Loss: 0.0154\n",
      "Epoch: 016/030 | Batch 0033/0136 | Loss: 0.4282\n",
      "Epoch: 016/030 | Batch 0065/0136 | Loss: 0.4334\n",
      "Epoch: 016/030 | Batch 0097/0136 | Loss: 0.4334\n",
      "Epoch: 016/030 | Batch 0129/0136 | Loss: 0.4531\n",
      "Epoch: 017/030 | Batch 0001/0136 | Loss: 0.0142\n",
      "Epoch: 017/030 | Batch 0033/0136 | Loss: 0.3305\n",
      "Epoch: 017/030 | Batch 0065/0136 | Loss: 0.4077\n",
      "Epoch: 017/030 | Batch 0097/0136 | Loss: 0.3715\n",
      "Epoch: 017/030 | Batch 0129/0136 | Loss: 0.3850\n",
      "Epoch: 018/030 | Batch 0001/0136 | Loss: 0.0103\n",
      "Epoch: 018/030 | Batch 0033/0136 | Loss: 0.3471\n",
      "Epoch: 018/030 | Batch 0065/0136 | Loss: 0.3145\n",
      "Epoch: 018/030 | Batch 0097/0136 | Loss: 0.3332\n",
      "Epoch: 018/030 | Batch 0129/0136 | Loss: 0.3280\n",
      "Epoch: 019/030 | Batch 0001/0136 | Loss: 0.0069\n",
      "Epoch: 019/030 | Batch 0033/0136 | Loss: 0.2755\n",
      "Epoch: 019/030 | Batch 0065/0136 | Loss: 0.2691\n",
      "Epoch: 019/030 | Batch 0097/0136 | Loss: 0.2727\n",
      "Epoch: 019/030 | Batch 0129/0136 | Loss: 0.2829\n",
      "Epoch: 020/030 | Batch 0001/0136 | Loss: 0.0060\n",
      "Epoch: 020/030 | Batch 0033/0136 | Loss: 0.2368\n",
      "Epoch: 020/030 | Batch 0065/0136 | Loss: 0.2435\n",
      "Epoch: 020/030 | Batch 0097/0136 | Loss: 0.2686\n",
      "Epoch: 020/030 | Batch 0129/0136 | Loss: 0.2534\n",
      "Epoch: 021/030 | Batch 0001/0136 | Loss: 0.0046\n",
      "Epoch: 021/030 | Batch 0033/0136 | Loss: 0.1971\n",
      "Epoch: 021/030 | Batch 0065/0136 | Loss: 0.2068\n",
      "Epoch: 021/030 | Batch 0097/0136 | Loss: 0.2342\n",
      "Epoch: 021/030 | Batch 0129/0136 | Loss: 0.2529\n",
      "Epoch: 022/030 | Batch 0001/0136 | Loss: 0.0049\n",
      "Epoch: 022/030 | Batch 0033/0136 | Loss: 0.1813\n",
      "Epoch: 022/030 | Batch 0065/0136 | Loss: 0.1632\n",
      "Epoch: 022/030 | Batch 0097/0136 | Loss: 0.1652\n",
      "Epoch: 022/030 | Batch 0129/0136 | Loss: 0.2123\n",
      "Epoch: 023/030 | Batch 0001/0136 | Loss: 0.0038\n",
      "Epoch: 023/030 | Batch 0033/0136 | Loss: 0.1376\n",
      "Epoch: 023/030 | Batch 0065/0136 | Loss: 0.1493\n",
      "Epoch: 023/030 | Batch 0097/0136 | Loss: 0.1465\n",
      "Epoch: 023/030 | Batch 0129/0136 | Loss: 0.1614\n",
      "Epoch: 024/030 | Batch 0001/0136 | Loss: 0.0079\n",
      "Epoch: 024/030 | Batch 0033/0136 | Loss: 0.1227\n",
      "Epoch: 024/030 | Batch 0065/0136 | Loss: 0.1264\n",
      "Epoch: 024/030 | Batch 0097/0136 | Loss: 0.1315\n",
      "Epoch: 024/030 | Batch 0129/0136 | Loss: 0.1462\n",
      "Epoch: 025/030 | Batch 0001/0136 | Loss: 0.0034\n",
      "Epoch: 025/030 | Batch 0033/0136 | Loss: 0.1089\n",
      "Epoch: 025/030 | Batch 0065/0136 | Loss: 0.1034\n",
      "Epoch: 025/030 | Batch 0097/0136 | Loss: 0.1072\n",
      "Epoch: 025/030 | Batch 0129/0136 | Loss: 0.1230\n",
      "Epoch: 026/030 | Batch 0001/0136 | Loss: 0.0017\n",
      "Epoch: 026/030 | Batch 0033/0136 | Loss: 0.0936\n",
      "Epoch: 026/030 | Batch 0065/0136 | Loss: 0.0969\n",
      "Epoch: 026/030 | Batch 0097/0136 | Loss: 0.1003\n",
      "Epoch: 026/030 | Batch 0129/0136 | Loss: 0.1008\n",
      "Epoch: 027/030 | Batch 0001/0136 | Loss: 0.0022\n",
      "Epoch: 027/030 | Batch 0033/0136 | Loss: 0.0770\n",
      "Epoch: 027/030 | Batch 0065/0136 | Loss: 0.0834\n",
      "Epoch: 027/030 | Batch 0097/0136 | Loss: 0.0931\n",
      "Epoch: 027/030 | Batch 0129/0136 | Loss: 0.0962\n",
      "Epoch: 028/030 | Batch 0001/0136 | Loss: 0.0020\n",
      "Epoch: 028/030 | Batch 0033/0136 | Loss: 0.0704\n",
      "Epoch: 028/030 | Batch 0065/0136 | Loss: 0.0779\n",
      "Epoch: 028/030 | Batch 0097/0136 | Loss: 0.0924\n",
      "Epoch: 028/030 | Batch 0129/0136 | Loss: 0.0894\n",
      "Epoch: 029/030 | Batch 0001/0136 | Loss: 0.0028\n",
      "Epoch: 029/030 | Batch 0033/0136 | Loss: 0.0656\n",
      "Epoch: 029/030 | Batch 0065/0136 | Loss: 0.0618\n",
      "Epoch: 029/030 | Batch 0097/0136 | Loss: 0.0737\n",
      "Epoch: 029/030 | Batch 0129/0136 | Loss: 0.0773\n",
      "Epoch: 030/030 | Batch 0001/0136 | Loss: 0.0040\n",
      "Epoch: 030/030 | Batch 0033/0136 | Loss: 0.0602\n",
      "Epoch: 030/030 | Batch 0065/0136 | Loss: 0.0560\n",
      "Epoch: 030/030 | Batch 0097/0136 | Loss: 0.0676\n",
      "Epoch: 030/030 | Batch 0129/0136 | Loss: 0.0760\n"
     ]
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr = 0.001)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr = 0.001)\n",
    "    num_epochs = 30\n",
    "    #running_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            pad_input_seqs, input_seq_lengths, pad_target_seqs = batch\n",
    "            batch_size = pad_input_seqs.size(1)\n",
    "            pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "            encoder_hidden = encoder.init_hidden(batch_size).to(device)\n",
    "            _, encoder_hidden = encoder(pad_input_seqs, input_seq_lengths, encoder_hidden)            \n",
    "\n",
    "            teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            decoder_outputs, decoder_hidden = decoder(encoder_hidden, pad_target_seqs, teacher_forcing=teacher_forcing)\n",
    "            loss = 0\n",
    "            for j in range(decoder_outputs.size(0)):\n",
    "                loss += F.nll_loss(decoder_outputs[j], pad_target_seqs[j], ignore_index=0)\n",
    "            loss = loss / decoder_outputs.size(0)\n",
    "            loss.backward()\n",
    "\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()    \n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #if not epoch == 30:\n",
    "                #print(f'Iteration {iteration} | Loss {loss.item():.2f}\\n\\n'))\n",
    "   \n",
    "            if not batch_idx % 32:\n",
    "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx+1, \n",
    "                     len(trainloader), running_loss/32))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b0593ddb524b6520a01f1bbbcce4b5f",
     "grade": false,
     "grade_id": "cell-cb9033683841ebab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 5_encoder.pth.\n",
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 5_decoder.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(encoder, '5_encoder.pth')\n",
    "    tools.save_model(decoder, '5_decoder.pth')\n",
    "else:\n",
    "    hidden_size = 256\n",
    "    encoder = Encoder(trainset.input_lang.n_words, embed_size, hidden_size)\n",
    "    tools.load_model(encoder, '5_encoder.pth', device)\n",
    "    \n",
    "    decoder = Decoder(trainset.output_lang.n_words, embed_size, hidden_size)\n",
    "    tools.load_model(decoder, '5_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f3ba6e63d83529028b6a4aab2ee7d9f",
     "grade": true,
     "grade_id": "test_accuracy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bdb21ced817fe9f7a518f36c84cb672",
     "grade": false,
     "grade_id": "cell-25e4072e5588afaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Next we need to implement a function that converts a source sequence to an output sequence using the trained sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cb09b313f212bc56fdf821e70dd43dc",
     "grade": false,
     "grade_id": "translate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, src_seq):\n",
    "    \"\"\"Translate given sentence src_seq using trained encoder and decoder.\n",
    "    \n",
    "    Args:\n",
    "      encoder (Encoder): Trained encoder.\n",
    "      decoder (Decoder): Trained decoder.\n",
    "      src_seq of shape (src_seq_length,): LongTensor of word indices of the source sequence.\n",
    "    \n",
    "    Returns:\n",
    "      out_seq of shape (out_seq_length,): LongTensor of word indices of the output sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        hidden = encoder.init_hidden().to(device)\n",
    "        src_seq = src_seq.view(-1 , 1).to(device)\n",
    "        output, hidden = encoder(src_seq, [src_seq.size(0)], hidden)\n",
    "        output,hidden = decoder(hidden)\n",
    "        decoder_outputs = torch.max(output, dim = 2)[1]\n",
    "        \n",
    "    return decoder_outputs\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78d9e5ee11bb14a762ffcd79b21b595d",
     "grade": false,
     "grade_id": "cell-054e0c165e7f2048",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    src_seq = torch.tensor([1, 2, 3, 4]).to(device)\n",
    "    out_seq = translate(encoder, decoder, src_seq)\n",
    "    assert out_seq.shape[0] <= MAX_LENGTH, \\\n",
    "        f\"Too long output sequence: tgt_seq.shape[0]={tgt_seq.shape[0]}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ee9a8caddfcc99fe38ddcf19378a80",
     "grade": false,
     "grade_id": "cell-92c1efeca7ee36f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us now translate random sentences from the training set and print the source, target, and produced output.\n",
    "\n",
    "If you trained the model well enough, the model should memorize the training data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7a78dc6d23c6a6a769a043cc1fca517",
     "grade": false,
     "grade_id": "cell-fe6996ecee284354",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate training data:\n",
      "-----------------------------\n",
      "SRC: elles sont furieuses apres toi . EOS\n",
      "TGT: they re mad at you . EOS\n",
      "OUT: they re mad at you . EOS EOS EOS EOS\n",
      "\n",
      "SRC: je pars en turquie demain . EOS\n",
      "TGT: i m off to turkey tomorrow . EOS\n",
      "OUT: i m off to turkey tomorrow . EOS EOS EOS\n",
      "\n",
      "SRC: ils sont nos invites . EOS\n",
      "TGT: they are our guests . EOS\n",
      "OUT: they are our guests . EOS EOS EOS EOS EOS\n",
      "\n",
      "SRC: tu me decois vraiment . EOS\n",
      "TGT: i m very disappointed in you . EOS\n",
      "OUT: i m very disappointed in you . EOS EOS EOS\n",
      "\n",
      "SRC: je viens de roumanie . EOS\n",
      "TGT: i m from romania . EOS\n",
      "OUT: i m from romania . EOS EOS EOS EOS EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate random sentences from the training set\n",
    "print('Translate training data:')\n",
    "print('-----------------------------')\n",
    "for i in range(5):\n",
    "    src_sentence, tgt_sentence = trainset[np.random.choice(len(trainset))]\n",
    "    print('SRC:', ' '.join(trainset.input_lang.index2word[i.item()] for i in src_sentence))\n",
    "    print('TGT:', ' '.join(trainset.output_lang.index2word[i.item()] for i in tgt_sentence))\n",
    "    out_sentence = translate(encoder, decoder, src_sentence)\n",
    "    print('OUT:', ' '.join(trainset.output_lang.index2word[i.item()] for i in out_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b10f912e08032dc9257adcb0bbb09ba3",
     "grade": false,
     "grade_id": "cell-c35e175e1fc511bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we translate random sentences from the test set. A well-trained model should output sentences that look similar to the target ones. The mistakes are usually done for words that were rare in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f268ad9539d79bc2af8e349a2a94ff99",
     "grade": false,
     "grade_id": "cell-e27f5e4329673f0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "testset = TranslationDataset(data_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24212f67f7d31016e3a85e3e82f79853",
     "grade": false,
     "grade_id": "cell-c1cafaf3ca027d6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate test data:\n",
      "-----------------------------\n",
      "SRC: ils mentent tous . EOS\n",
      "TGT: they re all lying . EOS\n",
      "OUT: they re all watching . EOS EOS EOS EOS EOS\n",
      "\n",
      "SRC: j ai peur de mourir . EOS\n",
      "TGT: i m afraid of death . EOS\n",
      "OUT: i am afraid of dying . EOS EOS EOS EOS\n",
      "\n",
      "SRC: il est nerveux d aller en amerique . EOS\n",
      "TGT: he is anxious to go to america . EOS\n",
      "OUT: he is nervous to the two team . EOS EOS\n",
      "\n",
      "SRC: elle a arrete notre dispute . EOS\n",
      "TGT: she stopped our fighting . EOS\n",
      "OUT: she slipped into her . . EOS EOS EOS EOS\n",
      "\n",
      "SRC: tu es vraiment magnifique . EOS\n",
      "TGT: you re really gorgeous . EOS\n",
      "OUT: you are really annoying . EOS EOS EOS EOS EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Translate test data:')\n",
    "print('-----------------------------')\n",
    "for i in range(5):\n",
    "    src_sentence, tgt_sentence = testset[np.random.choice(len(testset))]\n",
    "    print('SRC:', ' '.join(testset.input_lang.index2word[i.item()] for i in src_sentence))\n",
    "    print('TGT:', ' '.join(testset.output_lang.index2word[i.item()] for i in tgt_sentence))\n",
    "    out_sentence = translate(encoder, decoder, src_sentence)\n",
    "    print('OUT:', ' '.join(testset.output_lang.index2word[i.item()] for i in out_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
