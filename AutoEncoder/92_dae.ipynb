{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0461999c44bfc2ba7c7336090f65db94",
     "grade": false,
     "grade_id": "cell-be8c5c03905df198",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 1\n",
    "<br>\n",
    "<b>Deadline:</b> May 13, 2020 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 9.2. Denoising autoencoders\n",
    "\n",
    "The goal of this exercise is to get familiar with *denoising* autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83bbf952bc63ef66213753c4d3df4ceb",
     "grade": true,
     "grade_id": "cell-4c5ad871b433468b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import normal\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a656abba884bea225c43a917fbd0951",
     "grade": false,
     "grade_id": "cell-59bd1af0b867a73f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16d54f46952cc41603e69e73aea98e8e",
     "grade": false,
     "grade_id": "cell-b2b6a9c89bb934a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "In this exercise, we will use MNIST to create a new dataset (that we call varianceMNIST). In the new dataset, the information about the shapes of the digits is represented in the variances of the pixel intensities and not in the pixel intensities (like in MNIST). We use a custom `transform.Lambda()` to generate the dataset. Note that our dataset contains an infinite amount of samples because we generate different noise instances every time we request the data. The number of shapes is of course limited to the number of digits in the MNIST dataset.\n",
    "\n",
    "This is a challenging dataset and a plain bottleneck autoencoder (from Exercise 9.1) with a mean-squared error (MSE) loss cannot encode useful information in the bottleneck layer. However, a denoising autoencoder trained with an MSE loss is able to encode the shapes of the digits in the bottleneck layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "019586b617be8bbc28c79a50437ff101",
     "grade": false,
     "grade_id": "cell-e59e6a0054c13c82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use varianceMNIST data in this exercise\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Lambda(lambda x: x * torch.randn_like(x))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8373dd37832ca59582d6966b0bbfdfad",
     "grade": false,
     "grade_id": "cell-9e0ac40239400446",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We visualize some random training samples in the cell below. As you can see, we can quite easily identify the shapes and recognize the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84f334d2457f04f376e51df1f5770b7e",
     "grade": false,
     "grade_id": "cell-3b2f4813f230f675",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = iter(trainloader).next()\n",
    "tools.plot_images(images[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50648356cc07337524c37315d7fb0172",
     "grade": false,
     "grade_id": "cell-64dcf0d0caa30c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Denoising autoencoder (DAE)\n",
    "\n",
    "### Optimal denoising function\n",
    "\n",
    "Suppose we corrupt an image $\\mathbf{x}$ of the varianceMNIST dataset with a zero-mean Gaussian noise with standard deviation $\\sigma_n$. For a given clean pixel value $x$, the corrupted value $\\tilde{x}$ is thus produced as:\n",
    "$$\n",
    "\\tilde{x} = x + n, \\qquad n \\sim \\mathcal{N}(0, \\sigma^2_n)\n",
    "$$\n",
    "Please do not confuse the corruption process with the generative process of the varianceMNIST dataset. We assume that the varianceMNIST dataset is given to us, while we are free to select any corruption process to train a DAE. In this experiment, we choose Gaussian corruption.\n",
    "\n",
    "Knowing the generative process of the varianceMNIST dataset (which is a bit of cheating because we usually do not know the data generative process), we can compute the optimal denoising function which produces an estimate of the clean pixel value $x$ given corrupted value $\\tilde{x}$:\n",
    "$$\n",
    "g(\\tilde{x}) = \\tilde{x} \\: \\text{sigmoid}(f(\\sigma_x^2, \\sigma_n^2))\n",
    "$$\n",
    "where $f$ is some function of the variance $\\sigma^2_x$ of a pixel intensity in the varianceMNIST dataset and the variance $\\sigma^2_n$ of the corruption noise.\n",
    "\n",
    "\n",
    "In the cell below, your task is to implement a denoising autoencoder (DAE) which can learn to approximate the optimal denoising function shown above.\n",
    "* Our DAE will be trained to learn the optimal denoising function $g(\\tilde{x})$. In each training iteration, we feed corrupted images $\\tilde{\\mathbf{x}}$ to the inputs of the DAE and provide the corresponding clean images $\\mathbf{x}$ as the targets for the DAE outputs.\n",
    "* To learn useful representations (the shapes of the digits for the varianceMNIST dataset), our DAE will have a bottleneck layer with `n_components` elements. It is the output of the encoder.\n",
    "* We are not going to use values of $\\sigma_x^2$ and $\\sigma_n^2$ inside the DAE: The value of $\\sigma_x^2$ we simply do not know. We know the value of $\\sigma_n^2$ (because we select the corruption process) but we are not going to use that value in the computations of the denoising function.\n",
    "* Look carefully at the structure of the optimal denoising function. We can select the architecture of the DAE that makes it easy to perform the computations needed for optimal denoising.\n",
    "\n",
    "The proposed architecture for the DAE:\n",
    "* Encoder:\n",
    "    * `Conv2d` layer with kernel size 5 with 6 output channels, followed by ReLU\n",
    "    * `Conv2d` layer with kernel size 5 with 16 output channels, followed by ReLU\n",
    "    * Fully-connected layer with 250 output features, followed by ReLU\n",
    "    * Fully-connected layer with `n_components`\n",
    "* Decoder:\n",
    "    * Fully-connected layer with 250 output features, followed by ReLU\n",
    "    * Fully-connected layer with 250 input features, followed by ReLU\n",
    "    * `ConvTranspose2d` layer with kernel size 5 with 16 input channels, followed by ReLU\n",
    "    * `ConvTranspose2d` layer with kernel size 5 with 6 input channels\n",
    "\n",
    "Notes:\n",
    "* The exact architecture is not tested in this notebook. The above description is not full, you need to add some missing connections using the knowledge of the form of the optimal denoising function.\n",
    "* Please use recommended convolutional layers in the encoder and the decoder. If the autoencoder consists of only fully-connected layers, the learning problem is harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e87c77743f014e48470f2d7845d10af",
     "grade": false,
     "grade_id": "DAE",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DAE(nn.Module):\n",
    "    def __init__(self, n_components=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_components (int): Number of outputs in the bottleneck layer.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super(DAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels= 6, kernel_size = 5, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 6, out_channels= 16, kernel_size = 5, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, n_components)\n",
    "        )\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Linear(n_components, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 256),\n",
    "            nn.ReLU(),\n",
    "            #nn.ConvTrasnpose2d(in_channels = 16, ou_channels = 6, kernel_size = 5)\n",
    "        )\n",
    "        self.decoder2 =nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 16, out_channels = 6, kernel_size = 5, stride = 2, output_padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels = 6, out_channels = 1, kernel_size = 5, stride = 2, output_padding = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, n_channels=1, width, height): Examples corrupted with noise.\n",
    "\n",
    "        Returns:\n",
    "          z of shape (batch_size, n_components): Outputs of the bottleneck layer.\n",
    "          denoised_x of shape (batch_size, n_channels=1, width, height): Denoised examples.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x_in = x.clone()\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder1(z)\n",
    "        #y = print(y.size)\n",
    "        #y = y.view(16, 16)\n",
    "        _y = np.sqrt(y.size(1)/ 16).astype(int)\n",
    "        y = y.reshape(-1, 16, _y , _y)\n",
    "        y = self.decoder2(y)\n",
    "        \n",
    "        return z, x_in * torch.sigmoid(y)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "762ad40a5382484ccd26e888e90e3868",
     "grade": false,
     "grade_id": "cell-00fa5a667ce568cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_DAE_shapes():\n",
    "    n_components = 2\n",
    "    dae = DAE(n_components)\n",
    "\n",
    "    x = torch.randn(3, 1, 28, 28)\n",
    "    z, y = dae(x)\n",
    "    assert z.shape == torch.Size([3, n_components]), f\"Bad z.shape: {z.shape}\"\n",
    "    assert y.shape == x.shape, \"Bad y.shape: {y.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_DAE_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77dcf973d8738d2241d863fce4f7c2e8",
     "grade": false,
     "grade_id": "cell-b0025c31387e75cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train a denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d993412a5a1f5975a364f1f810e798c",
     "grade": false,
     "grade_id": "cell-78dea48b9207c439",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten()\n",
       "    (5): Linear(in_features=256, out_features=250, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): Linear(in_features=250, out_features=10, bias=True)\n",
       "  )\n",
       "  (decoder1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=250, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=250, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder2): Sequential(\n",
       "    (0): ConvTranspose2d(16, 6, kernel_size=(5, 5), stride=(2, 2), output_padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(6, 1, kernel_size=(5, 5), stride=(2, 2), output_padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an autoencoder\n",
    "n_components = 10\n",
    "dae = DAE(n_components)\n",
    "dae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43719bd8329f33bea79acadd186912b1",
     "grade": false,
     "grade_id": "cell-05c2281f8c2211fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below. Training proceeds similarly to the standard bottleneck autoencoder. The difference is that the encoder gets *corrupted* training images as inputs and the targets are the varianceMNIST digits without the corruption noise.\n",
    "\n",
    "The recommended hyperparameters:\n",
    "* Corruption of varianceMNIST images with **additive** Gaussian noise with zero mean and standard deivation $\\sigma_n=0.2$.\n",
    "* Adam optimizer with learning rate 0.001\n",
    "* MSE loss\n",
    "\n",
    "Hints:\n",
    "- Training usually converges fast, a couple of epochs should suffice.\n",
    "- The loss at convergence should be close to 0.009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "172280b79837ad6192e61ef225d16ea9",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 ===\n",
      "Avg Train Loss: 0.017022830231006883\n",
      "=== Epoch: 1 ===\n",
      "Avg Train Loss: 0.01186270978588267\n",
      "=== Epoch: 2 ===\n",
      "Avg Train Loss: 0.010107903250394274\n",
      "=== Epoch: 3 ===\n",
      "Avg Train Loss: 0.00976464104817983\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    criteron = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(dae.parameters(), lr=0.001)\n",
    "    train_loss = []\n",
    "    num_epoch = 4\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        print('=== Epoch: {} ==='.format(epoch))\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            noise =normal.Normal(0.0, 0.2)\n",
    "            noise = noise.sample()\n",
    "            x = torch.add(images, noise)\n",
    "            #print(x.shape)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            z, x_hat = dae(x)\n",
    "            loss = criteron(x_hat, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        '''\n",
    "        for i, data in enumerate(trainloader):\n",
    "            #print(images.size(1))\n",
    "            images, labels = data\n",
    "            \n",
    "            images = images.to(device)\n",
    "            \n",
    "            #add noise to image data\n",
    "            noisy_images  = images + 0.2 * torch.randn(*images.shape)\n",
    "            noisy_images = noisy_images.reshape(images.shape)\n",
    "            noisy_images = np.clip(noisy_images, 0., 1.)\n",
    "            \n",
    "            #noisy_images = noisy_images.to(device)\n",
    " \n",
    "            optimizer.zero_grad()\n",
    "            outputs = dae(noisy_images)\n",
    "            #dae_outputs = dae(images)           \n",
    "            loss = criteron(outputs, images)\n",
    "            loss.backward()\n",
    "            #train_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "           ''' \n",
    "        avg_loss = np.mean(train_loss[-i: ])\n",
    "        print('Avg Train Loss: {}'.format(avg_loss))\n",
    "\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "424f7e706baead7b6f31324f1eec5d52",
     "grade": false,
     "grade_id": "cell-d9871235237c49d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 9_dae.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(dae, '9_dae.pth')\n",
    "else:\n",
    "    dae = DAE(n_components=10)\n",
    "    tools.load_model(dae, '9_dae.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d0497cba0dda7555c86c847de8b8fb1",
     "grade": false,
     "grade_id": "cell-7953c9131c274027",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize embeddings\n",
    "\n",
    "Let us visualize the latent space in the cell below. If your DAE does a good job, you should clearly see ten clusters corresponding to the ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b7340d5da797517bd400d95fc6b8066",
     "grade": false,
     "grade_id": "cell-7b912a34da7ea0ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use t-SNE\n"
     ]
    }
   ],
   "source": [
    "tests.visualize_embeddings(lambda x: dae(x)[0], trainloader, n_samples=1000, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4ffe217eacbfe3c570b8ded01c07647",
     "grade": false,
     "grade_id": "cell-a12ae53cbe38abd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below, we denoise some test images using the trained DAE. If your DAE does a good job, it should remove noise from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c2a021cbb39dda0b73fdf6607ec577",
     "grade": false,
     "grade_id": "cell-9e0b910e916b4703",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_denoising(trainloader):\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images[:4].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corrupted_images = images + 0.2 * torch.randn_like(images)\n",
    "        z, reconstructions = dae(corrupted_images)\n",
    "    tools.plot_images(\n",
    "        torch.cat([corrupted_images, reconstructions]),\n",
    "        ncol=4, cmap=plt.cm.bwr, clim=[-3,3]\n",
    "    )\n",
    "\n",
    "plot_denoising(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "564e67c421adcc4ae45ddcbd397dae6d",
     "grade": false,
     "grade_id": "cell-4ed4c7d66516fa37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test the quality of the produced embeddings by classification\n",
    "\n",
    "We will test the quality of the produced encodings by training a simple linear regression classifier using the encoded images. If the classifier gives a reasonable accuracy, this is an evidence that we learned to represent the shapes of the digits in the bottleneck layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57772a6e6ab397a144284f09033d5741",
     "grade": false,
     "grade_id": "cell-e0d4533abca6846a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c1ba9a1e331a6bace0db5dcf15bea0d",
     "grade": false,
     "grade_id": "cell-7e181194d756063b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Encode data samples using the encoder\n",
    "def encode(dataset, dae):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        for images, labels_ in dataloader:\n",
    "            z, rec = dae(images.to(device))\n",
    "            embeddings.append(z)\n",
    "            labels.append(labels_)\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "    return embeddings, labels\n",
    "\n",
    "traincodes, trainlabels = encode(trainset, dae)  # traincodes is (60000, 10)\n",
    "testcodes, testlabels = encode(testset, dae)  # testcodes is (10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "957c1af8d354975b3d6d86b4e4fe725d",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with a linear classifier: 63.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Poor accuracy of the embeddings: classification accuracy is 63.82%",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7003f3cef4dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy with a linear classifier: %.2f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.83\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Poor accuracy of the embeddings: classification accuracy is %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Success'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Poor accuracy of the embeddings: classification accuracy is 63.82%"
     ]
    }
   ],
   "source": [
    "# Train a simple linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', max_iter=200)\n",
    "logreg.fit(traincodes.cpu(), trainlabels.cpu())\n",
    "\n",
    "predicted_labels = logreg.predict(testcodes.cpu())  # (10000,)\n",
    "\n",
    "accuracy = np.sum(testlabels.cpu().numpy() == predicted_labels) / predicted_labels.size\n",
    "print('Accuracy with a linear classifier: %.2f%%' % (accuracy*100))\n",
    "assert accuracy > .83, \"Poor accuracy of the embeddings: classification accuracy is %.2f%%\" % (accuracy*100)\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3b1a9f1756b62c370ba87c8f496a926",
     "grade": false,
     "grade_id": "cell-49dd30e7e1be67c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "In this exercise, we trained a denoising autoencoder to encode meaningful information in the bottleneck layer. The codes produced in the bottleneck layer are only 10-dimensional but they can represent useful information present in the original $28 \\times 28 = 784$-dimensional images. You can try to use in this task a plain bottleneck autoencoder (trained without the corruption process) with MSE loss and you will see that it fails to develop useful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
