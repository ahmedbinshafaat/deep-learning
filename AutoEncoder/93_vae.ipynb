{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fef832b13430ea7fbc0aec6ebd922197",
     "grade": false,
     "grade_id": "cell-1991d883666dfe20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 2\n",
    "<br>\n",
    "<b>Deadline:</b> May 13, 2020 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 9.3. Variational autoencoders\n",
    "\n",
    "The goal of this exercise is to get familiar with *variational* autoencoders. The model was presented by [Kigma and Welling, 2013](https://arxiv.org/pdf/1312.6114.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83bbf952bc63ef66213753c4d3df4ceb",
     "grade": true,
     "grade_id": "cell-4c5ad871b433468b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3f18d508eaf8a4db74a35943bce651d",
     "grade": false,
     "grade_id": "cell-0b293facc9f5809f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The models are always evaluated on CPU\n",
    "if skip_training:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65ab9b764d13f319415361311da996e5",
     "grade": false,
     "grade_id": "cell-f220938cfeb3ff3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "In this exercise, we will use the same varianceMNIST dataset from Exercise 9.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4204377f6144b32470761fa0b03b9ec",
     "grade": false,
     "grade_id": "cell-e0f9dc3b448cdf86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use varianceMNIST data in this exercise\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Lambda(lambda x: x * torch.randn_like(x))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "043476d483ae788e347f6c5dfec88d7f",
     "grade": false,
     "grade_id": "cell-c3b9b6bf72ae75fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We visualize some random training samples in the cell below. As you can see, we can quite easily recognize the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c2523473cb0bd70e73f71f119a1fce8",
     "grade": false,
     "grade_id": "cell-29d84db735af3095",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = iter(trainloader).next()\n",
    "tools.plot_images(images[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cc23aaa1a240aea105d3d172b490a0c",
     "grade": false,
     "grade_id": "cell-ff4e6ea5533a0eb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Variational autoencoder (VAE)\n",
    "\n",
    "In this exercise, we will assume the following generative model for the data:\n",
    "* the latent codes are normally distributed:\n",
    "$$\n",
    "  p(z) = \\mathcal{N}(z \\mid 0, I)\n",
    "$$\n",
    "where $I$ is the identity matrix.\n",
    "* the data are produced from the latent codes as follows:\n",
    "$$\n",
    "  p(x \\mid z) =\\mathcal{N}\\left(x \\mid \\mu_x(z), \\:\\text{diag}(\\sigma^2_x(z)) \\right)\n",
    "$$\n",
    "where $\\mu_x(z)$ and $\\sigma^2_x(z)$ are some deterministic functions that we need to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bc2c76f081978c46ac7af9a298d7b0f",
     "grade": false,
     "grade_id": "cell-e852904e4780e6c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "In the cell below, your task is to implement the encoder of a VAE.\n",
    "The proposed architecture of the encoder is similar to the encoder from Exercise 9.2:\n",
    "* `Conv2d` layer with kernel size 5 with 6 output channels, followed by ReLU\n",
    "* `Conv2d` layer with kernel size 5 with 16 output channels, followed by ReLU\n",
    "* Fully-connected layer with 250 output features, followed by ReLU\n",
    "* Two heads: each is a fully-connected layer with `n_components` elements.\n",
    "\n",
    "The two heads are needed to produce two outputs of the encoder:\n",
    "* means $\\mu_z$ of the approximate distribution of the latent code $\\bar z$\n",
    "* log-variance $\\tilde z$ of the approximate distribution of the latent code $z$.\n",
    "To guarantee that the variance is positive, we parameterize it as $\\sigma_z^2 = \\exp(\\tilde z)$.\n",
    "\n",
    "Note: The exact architecture is not tested in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f80f0550b688bf4619c1e09608efff81",
     "grade": false,
     "grade_id": "Encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_components (int): Number of elements in produced codes.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 250),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.mu = nn.Linear(16*20*250, n_components)\n",
    "        self.logvar = nn.Linear(16*20*250, n_components)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, n_channels=1, width, height): Examples to encode.\n",
    "\n",
    "        Returns:\n",
    "          z_mean of shape (batch_size, n_components): Means of the approximate distributions of the codes.\n",
    "          z_logvar of shape (batch_size, n_components): Log-variances of the approximate distributions of the codes.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), x.size(1)*x.size(2) * x.size(3))\n",
    "        \n",
    "        mu, logvar = self.mu(x), self.logvar(x)\n",
    "        x = self.sample(mu, logvar)\n",
    "        \n",
    "        return x, logvar\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def sample(self, z_mean, z_logvar):\n",
    "        \"\"\"Draw one sample from the posterior of the latent codes described by given parameters.\n",
    "        This is needed for the re-parameterization trick.\n",
    "        \n",
    "        Args:\n",
    "          z_mean of shape (batch_size, n_components): Means of the approximate distributions of the codes.\n",
    "          z_logvar of shape (batch_size, n_components): Log-variance of the approximate distributions of the codes.\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (batch_size, n_components): Drawn samples.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        var = torch.exp(z_logvar)\n",
    "        eps = torch.randn_like(var)\n",
    "\n",
    "        return z_mean + eps * var\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6efe9577359a18b003ef06db148e9ea",
     "grade": false,
     "grade_id": "cell-67b29859e9438990",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_Encoder_shapes():\n",
    "    n_components = 10 # dimensional latent space\n",
    "    encoder = Encoder(n_components=n_components)\n",
    "\n",
    "    x = torch.randn(3, 1, 28, 28)\n",
    "    mu, logsigma = encoder(x)\n",
    "    assert mu.shape == torch.Size([3, n_components]), f\"Bad mu.shape: {mu.shape}\"\n",
    "    assert logsigma.shape == torch.Size([3, n_components]), f\"Bad logsigma.shape: {logsigma.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75418fdce78708b55754acd51b40dae5",
     "grade": false,
     "grade_id": "cell-e2ac1b8af3aa420b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_Encoder_sample():\n",
    "    n_components = 10 # dimensional latent space\n",
    "    encoder = Encoder(n_components=n_components)\n",
    "\n",
    "    z_mean = torch.zeros(3, n_components)\n",
    "    z_logvar = torch.log(2*torch.ones(3, n_components))\n",
    "    z = encoder.sample(z_mean, z_logvar)\n",
    "    assert z.shape == z_mean.shape, f\"Bad z.shape: {z.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77df1ee89278700d2e678f8a7f541a48",
     "grade": false,
     "grade_id": "cell-fb71617d43a06e36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Kullback-Leibler divergence loss\n",
    "\n",
    "One term of the loss function minimized during training of a VAE is the Kullback-Leibler divergence between the approximate distribution of the latent codes $q(z) = \\mathcal{N}(z \\mid \\mu_z, \\sigma^2_z)$ and the prior distribution $p(z) = \\mathcal{N}(z \\mid 0, I)$:\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N \\int q(z_i) \\log \\frac{q(z_i)}{p(z_i)} dz_i\n",
    "$$\n",
    "where $N$ is the number of samples (batch size in our implementation).\n",
    "\n",
    "We will implement this loss function in the cell below.\n",
    "\n",
    "Note: Please do **not** use functions from `torch.distributions` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4f8300a2b35d5a9edaa11e0f163013b",
     "grade": false,
     "grade_id": "loss_kl",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_kl(z_mean, z_logvar):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      z_mean of shape (batch_size, n_components): Means of the approximate distributions of the codes.\n",
    "      z_logvar of shape (batch_size, n_components): Log-variance of the approximate distributions of the codes.\n",
    "    \n",
    "    Returns:\n",
    "      loss (torch scalar): Kullback-Leibler divergence.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    x= z_mean.shape[0]\n",
    "    y= z_mean.shape[1]\n",
    "    KLD = -0.5* torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) / x\n",
    "    return KLD\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcd060012b8abdec87cb1b9af03c7fe7",
     "grade": true,
     "grade_id": "test_loss_kl",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_loss_kl():\n",
    "    n_components = 2\n",
    "    z_mean = torch.zeros(3, n_components)\n",
    "    z_logvar = torch.log(2*torch.ones(3, n_components))\n",
    "    loss = loss_kl(z_mean, z_logvar)\n",
    "    expected = torch.tensor(0.3068528175354004)\n",
    "    print('loss:', loss.item())\n",
    "    print('expected:', expected.item())\n",
    "    assert torch.allclose(loss, expected, atol=1e-5), \"loss does not match expected value.\"\n",
    "    print('Success')\n",
    "\n",
    "test_loss_kl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66924777f9826c3ab16c4ffc55edf732",
     "grade": false,
     "grade_id": "cell-d0bf566448126c1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder computes the predictive distribution of the data given latent code $z$ according to our\n",
    "assumed generative model:\n",
    "$$\n",
    "  p(x \\mid z) = \\mathcal{N}\\left(x \\mid \\mu_x(z), \\sigma^2_x(z) \\right)\n",
    "$$\n",
    "where $\\mu_x(z)$ and $\\sigma^2_x(z)$ are some deterministic functions that we need to learn.\n",
    "\n",
    "The proposed architecture of the decoder is similar to the one from Excercise 9.2:\n",
    "* Fully-connected layer with 250 output features, followed by ReLU\n",
    "* Fully-connected layer with 250 input features, followed by ReLU\n",
    "* `ConvTranspose2d` layer with kernel size 5 with 16 input channels, followed by ReLU\n",
    "* Two heads made of `ConvTranspose2d` layer with kernel size 5 with 6 input channels.\n",
    "\n",
    "The two heads are needed to produce two outputs of the decoder:\n",
    "* means $\\mu_x$ of the predictive distribution of the data\n",
    "* log-variance $\\tilde x$ of the predictive distribution of the data.\n",
    "To guarantee that the variance is positive, we parameterize it as $\\sigma_x^2 = \\exp(\\tilde x)$.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "In practice, learning the proposed generative model is difficult for the varianceMNIST dataset. The problem is that the background pixels have zero variances, which corresponds to infinitely low loss values. Thus, training may concentrate entirely on modeling the variance of the background pixels. To prevent this, we define the minimum allowed value of the predictive variance $\\tilde x$ and save it in the model as\n",
    "```\n",
    "    self.register_buffer('min_logvar', -6 * torch.ones(1))\n",
    "```\n",
    "We need to use `register_buffer` to make sure that the variable is on the same device as the trained parameters of the model. We can use this code in the forward function to limit the predicted variance by `self.min_logvar`:\n",
    "```\n",
    "logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faf092c213b34354e527f934c26f6b0e",
     "grade": false,
     "grade_id": "decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_components (int): Number of elements in input codes.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(250, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 6, kernel_size=5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv1=nn.ConvTranspose2d(6, 1, kernel_size=5)\n",
    "        self.conv2=nn.ConvTranspose2d(6, 1, kernel_size=5)\n",
    "\n",
    "        self.fc2 = nn.Linear(n_components, 16*20*250)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, n_components): Input codes.\n",
    "\n",
    "        Returns:\n",
    "          y_mean of shape (batch_size, n_components): Means of the probability distributions describing the\n",
    "                                                      data examples that correspond to the given codes.\n",
    "          y_logvar of shape (batch_size, n_components): Log-variances of the probability distributions describing the\n",
    "                                                        data examples that correspond to the given codes.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(x.size(0),16,20, 250)\n",
    "        x = self.decoder(x)\n",
    "        x_hat = self.conv1(x)\n",
    "        mu = self.conv2(x)\n",
    "        return x_hat,mu\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Decoder_shapes():\n",
    "    n_components = 10 # dimensional latent space\n",
    "    decoder = Decoder(n_components=n_components)\n",
    "\n",
    "    z = torch.randn(3, n_components)\n",
    "    y_mean, y_logvar = decoder(z)\n",
    "    y_shape = torch.Size([3, 1, 28, 28])\n",
    "    assert y_mean.shape == y_shape, \"Bad shape of y_mean: y_mean.shape={}\".format(y_mean.shape)\n",
    "    assert y_logvar.shape == y_shape, \"Bad shape of y_logvar: y_logvar.shape={}\".format(y_logvar.shape)\n",
    "    print('Success')\n",
    "\n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b1f586bd355faab788788b3ec3350ef",
     "grade": false,
     "grade_id": "cell-e452fa6333e86cb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Expected log-likelihood term\n",
    "\n",
    "The second term of the VAE loss function is minus log-likelihood estimated using sample $z_i$ from the approximate distribution $q(z_i)$ of the latent code that corresponds to training example $x_i$.\n",
    "\n",
    "$$\n",
    "- \\int q(z_i) \\log \\mathcal{N}\\left(x_i \\mid \\mu_x(z_i), \\:\\text{diag}(\\sigma^2_x(z_i))\\right) dz_i\n",
    "$$\n",
    "where $\\mathcal{N}(x_i)$ is a multivariate normal distribution over all pixel values of image $x_i$.\n",
    "\n",
    "Your task is to implement that function **without** constant terms\n",
    "$$\n",
    "\\frac{28 \\cdot 28}{2} \\log 2 \\pi\n",
    "$$\n",
    "that do not depend on $\\mu_x(z_i)$ or $\\sigma_x(z_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ec86ab777637ddb25fefbd0e599a11a",
     "grade": false,
     "grade_id": "loss_loglik",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_loglik(y_mean, y_logvar, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      y_mean of shape (batch_size, 1, 28, 28): Predictive mean of the VAE reconstruction of x.\n",
    "      y_logvar of shape (batch_size, 1, 28, 28): Predictive log-variance of the VAE reconstruction of x.\n",
    "      x of shape (batch_size, 1, 28, 28): Training samples.\n",
    "    \n",
    "    Returns:\n",
    "      loss (torch scalar): Expected log-likelihood loss.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    y=y_mean.shape[0]\n",
    "    loss = torch.sum(y_logvar + (x - y_mean)**2 / (torch.exp(y_logvar)))/(2*y)\n",
    "    \n",
    "    return loss\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c83cddd1b0e851d4e1406881b0e1ea5b",
     "grade": true,
     "grade_id": "test_loss_loglik",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_loss_loglik():\n",
    "    y_mean = torch.zeros(1, 1, 28, 28)\n",
    "    y_logvar = torch.log(2*torch.ones(1, 1, 28, 28))\n",
    "    y_logvar[:,:,:14,:] = torch.log(torch.ones(1, 1, 14, 28))\n",
    "\n",
    "    x = torch.zeros(1, 1, 28, 28)\n",
    "    x[:,:,:14,:] = torch.zeros(1, 1, 14, 28)\n",
    "\n",
    "    loss = loss_loglik(y_mean, y_logvar, x)\n",
    "    expected = torch.tensor(135.85682678222656)\n",
    "    \n",
    "    print('loss:', loss)\n",
    "    print('expected:', expected)\n",
    "    assert torch.allclose(loss, expected), \"loss does not match expected value.\"\n",
    "    print('Success')\n",
    "\n",
    "test_loss_loglik()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27ee32c4fd471ea2f5f2fe1a53182afa",
     "grade": false,
     "grade_id": "cell-9ca6445aad88892a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train a variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40908985c2e2acef6a8cbafcd865d733",
     "grade": false,
     "grade_id": "cell-3580d8de6f07ed5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a variational autoencoder\n",
    "n_components = 10 # dimensional latent space\n",
    "encoder = Encoder(n_components=n_components)\n",
    "decoder = Decoder(n_components=n_components)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89a83792247322d12d7506e833e44cd4",
     "grade": false,
     "grade_id": "cell-c1d47e180f13d2d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below. One iteration of the training loop process one mini-batch of data in the following way:\n",
    "* The encoder is used to compute approximate distributions $q(z)$ of the latent codes corresponding to the training samples.\n",
    "* One sample $z_i$ is drawn from each approximate posterior $q(z)$ (use function `Encoder.sample()` for that).\n",
    "* The decoder uses samples $z_i$ to compute the predictive distribution for the training examples.\n",
    "* The minimized loss is the sum of the KL-divergence loss `loss_kl()` and the expected log-likelihood loss `loss_loglik()` defined earlier.\n",
    "\n",
    "Implement the training loop in the cell below. The recommended hyperparameters:\n",
    "* Adam optimizer with learning rate 0.001\n",
    "* Number of epochs: 10\n",
    "\n",
    "Hints:\n",
    "- The loss at convergence should be close to -1760."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faf6819518f47043935cf238a18b325a",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Training loop\n",
    "if not skip_training:\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "    num_epochs = 10\n",
    "    train_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for i, (input, target) in enumerate(trainloader):\n",
    "            # Forward + Backward + Optimize\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            \n",
    "            z_mean, z_logvar = encoder(input)\n",
    "            y_mean,y_logvar = decoder(z_mean)\n",
    "            optimizer.zero_grad()\n",
    "            #      z_mean of shape (batch_size, n_components): Means of the approximate distributions of the codes.\n",
    "            #z_logvar of shape (batch_size, n_components): Log-variance of the approximate distributions of the codes.\n",
    "            \n",
    "            loss1 = loss_kl(z_mean, z_logvar)\n",
    "            \n",
    "            #y_mean of shape (batch_size, 1, 28, 28): Predictive mean of the VAE reconstruction of x.\n",
    "            #y_logvar of shape (batch_size, 1, 28, 28): Predictive log-variance of the VAE reconstruction of x.\n",
    "            #x of shape (batch_size, 1, 28, 28): Training samples.\n",
    "            loss2 = loss_loglik(y_mean,y_logvar,input)\n",
    "            #print(y_mean.shape,y_logvar.shape,target.shape)\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "    avg_loss = np.mean(train_loss[-i:])\n",
    "    print('Avg Train Loss: {}'.format(avg_loss))     \n",
    "        print('Epoch [{}/{}], Loss1:{:.4f},Loss2:{:.4f},Loss:{:.4f}'.format(epoch + 1, num_epochs, loss1.item(),loss2.item(),loss.item()))\n",
    "'''\n",
    "# Training loop\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
    "    train_loss = []\n",
    "    num_epoch = 10\n",
    "    for epoch in range(num_epoch):\n",
    "        print('=== Epoch: {} ==='.format(epoch))\n",
    "        for i, (input, target) in enumerate(trainloader):\n",
    "            # Forward + Backward + Optimize\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            z_mean, z_logvar = encoder(input)\n",
    "            y_mean,y_logvar = decoder(z_mean)\n",
    "            optimizer.zero_grad()\n",
    "            loss1 = loss_kl(z_mean, z_logvar)\n",
    "            loss2 = loss_loglik(y_mean,y_logvar,input)\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "        avg_loss = np.mean(train_loss[-i:])\n",
    "        print('Avg Train Loss: {}'.format(avg_loss))      \n",
    "    #raise NotImplementedError()\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9ad3cf5c68037ca094b754c4397e312",
     "grade": false,
     "grade_id": "cell-0aaedbfc914ebad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(encoder, '9_vae_encoder.pth')\n",
    "    tools.save_model(decoder, '9_vae_decoder.pth')\n",
    "else:\n",
    "    encoder = Encoder(n_components=10)\n",
    "    tools.load_model(encoder, '9_vae_encoder.pth', device)\n",
    "\n",
    "    decoder = Decoder(n_components=10)\n",
    "    tools.load_model(decoder, '9_vae_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df03e6a015bc497380adf237d7a7655e",
     "grade": false,
     "grade_id": "cell-6c9ae1497b99a8c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize embeddings\n",
    "\n",
    "Let us visualize the latent space in the cell below. If your VAE does a good job, you should clearly see ten clusters corresponding to the ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab5f46e25d6e2efb6e09b79a584fbb0c",
     "grade": false,
     "grade_id": "cell-63ea155392dc41d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tests.visualize_embeddings(lambda x: encoder(x)[0], trainloader, n_samples=1000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5661065e6812b83447497e41f51a4608",
     "grade": false,
     "grade_id": "cell-6154636b2213a564",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we visualize predictive variances of the model for each pixel.\n",
    "# For a well-trained VAE, the variances should capture the shapes of the digits.\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(trainloader)\n",
    "    images, _ = dataiter.next()\n",
    "    z_mean, z_logvar = encoder(images.to(device))\n",
    "    y_mean, y_logvar = decoder(z_mean)\n",
    "\n",
    "    # Visualize some data samples\n",
    "    tools.plot_images(images[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])\n",
    "    # Visualize corresponding predictive variance in the pixel space\n",
    "    tools.plot_images(torch.exp(y_logvar[:8]), ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88694ae55e162a5a24e448f7179293c6",
     "grade": false,
     "grade_id": "cell-bfa2da2d48cb1327",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate samples from VAE\n",
    "# Note that samples may not be of great quality because we did not optimize the architecture of our VAE.\n",
    "with torch.no_grad():\n",
    "    z = torch.randn((8, 10)).to(device)\n",
    "    x_mean, x_logvar = decoder(z)\n",
    "    x = x_mean + torch.exp(0.5 * x_logvar) * torch.randn_like(x_logvar)\n",
    "    tools.plot_images(x[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])\n",
    "    tools.plot_images(torch.exp(x_logvar[:8]), ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fd0c3a2c4743f176406b5ba995a6d6f",
     "grade": false,
     "grade_id": "cell-ca045d30afe73d6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Test the quality of the produced embeddings by classification\n",
    "\n",
    "We will test the quality of the produced encodings by training a classifier using the encoded images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7831b30b81c920308549fa300c9ca8f1",
     "grade": false,
     "grade_id": "cell-56d5d63f5cec4de3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3460c619d4606beb377f02635a6eb019",
     "grade": false,
     "grade_id": "cell-a12c49a87f4cfe60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Encode data samples using the VAE encoder\n",
    "def encode(dataset, dae):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        for images, labels_ in dataloader:\n",
    "            mu, logsigma = encoder(images.to(device))\n",
    "            embeddings.append(mu)\n",
    "            labels.append(labels_)\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "    return embeddings, labels\n",
    "\n",
    "traincodes, trainlabels = encode(trainset, encoder)  # traincodes is (60000, 10)\n",
    "testcodes, testlabels = encode(testset, encoder)  # testcodes is (10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0293e9c2fc61a14807da93a22909b005",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train a simple linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', max_iter=400)\n",
    "logreg.fit(traincodes.cpu(), trainlabels.cpu())\n",
    "\n",
    "predicted_labels = logreg.predict(testcodes.cpu())  # (10000,)\n",
    "\n",
    "# Compute accuracy of the linear classifier\n",
    "accuracy = np.sum(testlabels.cpu().numpy() == predicted_labels) / predicted_labels.size\n",
    "print('Accuracy with a linear classifier: %.2f%%' % (accuracy*100))\n",
    "assert accuracy > .8, \"Poor accuracy of the embeddings: classification accuracy is %.2f%%\" % (accuracy*100)\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40172e3e2ef83adcd8976fdedad2eb92",
     "grade": false,
     "grade_id": "cell-1f55c6b88b80405b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this exercise, we trained a variational autoencoder on a dataset in which useful information is in the variance of the pixel values and not the pixel values themselves. The code produced by the encoder of the VAE is only $10$-dimensional compared to the original $28 \\times 28 = 784$-dimensional data. Still, the codes capture well the shapes of the digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
